
*******************************
********** Parsed file ********
*******************************
[@@@ocaml.text
  " A generic lexical analyzer.\n\n\n   This module implements a simple 'standard' lexical analyzer, presented\n   as a function from character streams to token streams. It implements\n   roughly the lexical conventions of OCaml, but is parameterized by the\n   set of keywords of your language.\n\n\n   Example: a lexer suitable for a desk calculator is obtained by\n   {[     let lexer = make_lexer [\"+\";\"-\";\"*\";\"/\";\"let\";\"=\"; \"(\"; \")\"]  ]}\n\n   The associated parser would be a function from [token stream]\n   to, for instance, [int], and would have rules such as:\n\n   {[\n           let rec parse_expr = parser\n             | [< n1 = parse_atom; n2 = parse_remainder n1 >] -> n2\n           and parse_atom = parser\n             | [< 'Int n >] -> n\n             | [< 'Kwd \"(\"; n = parse_expr; 'Kwd \")\" >] -> n\n           and parse_remainder n1 = parser\n             | [< 'Kwd \"+\"; n2 = parse_expr >] -> n1+n2\n             | [< >] -> n1\n   ]}\n\n   One should notice that the use of the [parser] keyword and associated\n   notation for streams are only available through camlp4 extensions. This\n   means that one has to preprocess its sources {i e. g.} by using the\n   [\"-pp\"] command-line switch of the compilers.\n"]
type token =
  | Kwd of string 
  | Ident of string 
  | Int of int 
  | Float of float 
  | String of string 
  | Char of char [@@ocaml.doc
                   " The type of tokens. The lexical classes are: [Int] and [Float]\n   for integer and floating-point numbers; [String] for\n   string literals, enclosed in double quotes; [Char] for\n   character literals, enclosed in single quotes; [Ident] for\n   identifiers (either sequences of letters, digits, underscores\n   and quotes, or sequences of 'operator characters' such as\n   [+], [*], etc); and [Kwd] for keywords (either identifiers or\n   single 'special characters' such as [(], [}], etc). "]
val make_lexer : string list -> char Stream.t -> token Stream.t[@@ocaml.doc
                                                                 " Construct the lexer function. The first argument is the list of\n   keywords. An identifier [s] is returned as [Kwd s] if [s]\n   belongs to this list, and as [Ident s] otherwise.\n   A special character [s] is returned as [Kwd s] if [s]\n   belongs to this list, and cause a lexical error (exception\n   {!Stream.Error} with the offending lexeme as its parameter) otherwise.\n   Blanks and newlines are skipped. Comments delimited by [(*] and [*)]\n   are skipped as well, and can be nested. A {!Stream.Failure} exception\n   is raised if end of stream is unexpectedly reached."]

*******************************
****** GOSPEL translation *****
*******************************
(*@ open Gospelstdlib *)

[@@@ocaml.text
  " A generic lexical analyzer.\n\n\n   This module implements a simple 'standard' lexical analyzer, presented\n   as a function from character streams to token streams. It implements\n   roughly the lexical conventions of OCaml, but is parameterized by the\n   set of keywords of your language.\n\n\n   Example: a lexer suitable for a desk calculator is obtained by\n   {[     let lexer = make_lexer [\"+\";\"-\";\"*\";\"/\";\"let\";\"=\"; \"(\"; \")\"]  ]}\n\n   The associated parser would be a function from [token stream]\n   to, for instance, [int], and would have rules such as:\n\n   {[\n           let rec parse_expr = parser\n             | [< n1 = parse_atom; n2 = parse_remainder n1 >] -> n2\n           and parse_atom = parser\n             | [< 'Int n >] -> n\n             | [< 'Kwd \"(\"; n = parse_expr; 'Kwd \")\" >] -> n\n           and parse_remainder n1 = parser\n             | [< 'Kwd \"+\"; n2 = parse_expr >] -> n1+n2\n             | [< >] -> n1\n   ]}\n\n   One should notice that the use of the [parser] keyword and associated\n   notation for streams are only available through camlp4 extensions. This\n   means that one has to preprocess its sources {i e. g.} by using the\n   [\"-pp\"] command-line switch of the compilers.\n"]

type token =
| Kwd of string 
| Ident of string 
| Int of int 
| Float of float 
| String of string 
| Char of char [@@ocaml.doc
                 " The type of tokens. The lexical classes are: [Int] and [Float]\n   for integer and floating-point numbers; [String] for\n   string literals, enclosed in double quotes; [Char] for\n   character literals, enclosed in single quotes; [Ident] for\n   identifiers (either sequences of letters, digits, underscores\n   and quotes, or sequences of 'operator characters' such as\n   [+], [*], etc); and [Kwd] for keywords (either identifiers or\n   single 'special characters' such as [(], [}], etc). "]
  

val make_lexer : string list -> char Stream.t -> token Stream.t[@@ocaml.doc
                                                                 " Construct the lexer function. The first argument is the list of\n   keywords. An identifier [s] is returned as [Kwd s] if [s]\n   belongs to this list, and as [Ident s] otherwise.\n   A special character [s] is returned as [Kwd s] if [s]\n   belongs to this list, and cause a lexical error (exception\n   {!Stream.Error} with the offending lexeme as its parameter) otherwise.\n   Blanks and newlines are skipped. Comments delimited by [(*] and [*)]\n   are skipped as well, and can be nested. A {!Stream.Failure} exception\n   is raised if end of stream is unexpectedly reached."]


*******************************
********* Typed GOSPEL ********
*******************************
module genlex.mli

  Namespace: genlex.mli
    Type symbols
       token
      
    Logic Symbols
      function Char (_:char) : token
      function Float (_:float) : token
      function Ident (_:string) : token
      function Int (_:int) : token
      function Kwd (_:string) : token
      function String (_:string) : token
      
    Exception Symbols
      
    Namespaces
      
    Type Namespaces
      
  Signatures
    (*@ open Gospelstdlib *)
    
    [@@@ocaml.text
      " A generic lexical analyzer.\n\n\n   This module implements a simple 'standard' lexical analyzer, presented\n   as a function from character streams to token streams. It implements\n   roughly the lexical conventions of OCaml, but is parameterized by the\n   set of keywords of your language.\n\n\n   Example: a lexer suitable for a desk calculator is obtained by\n   {[     let lexer = make_lexer [\"+\";\"-\";\"*\";\"/\";\"let\";\"=\"; \"(\"; \")\"]  ]}\n\n   The associated parser would be a function from [token stream]\n   to, for instance, [int], and would have rules such as:\n\n   {[\n           let rec parse_expr = parser\n             | [< n1 = parse_atom; n2 = parse_remainder n1 >] -> n2\n           and parse_atom = parser\n             | [< 'Int n >] -> n\n             | [< 'Kwd \"(\"; n = parse_expr; 'Kwd \")\" >] -> n\n           and parse_remainder n1 = parser\n             | [< 'Kwd \"+\"; n2 = parse_expr >] -> n1+n2\n             | [< >] -> n1\n   ]}\n\n   One should notice that the use of the [parser] keyword and associated\n   notation for streams are only available through camlp4 extensions. This\n   means that one has to preprocess its sources {i e. g.} by using the\n   [\"-pp\"] command-line switch of the compilers.\n"]
    
    type token = Kwd of string
                 function Kwd (_:string) : token
              | Ident of string
                function Ident (_:string) : token
              | Int of int
                function Int (_:int) : token
              | Float of float
                function Float (_:float) : token
              | String of string
                function String (_:string) : token
              | Char of char
                function Char (_:char) : token
    
    
    val make_lexer :
    string list -> char Stream.t -> token Stream.t[@@ocaml.doc
                                                    " Construct the lexer function. The first argument is the list of\n   keywords. An identifier [s] is returned as [Kwd s] if [s]\n   belongs to this list, and as [Ident s] otherwise.\n   A special character [s] is returned as [Kwd s] if [s]\n   belongs to this list, and cause a lexical error (exception\n   {!Stream.Error} with the offending lexeme as its parameter) otherwise.\n   Blanks and newlines are skipped. Comments delimited by [(*] and [*)]\n   are skipped as well, and can be nested. A {!Stream.Failure} exception\n   is raised if end of stream is unexpectedly reached."]
    


*** OK ***

